import tensorflow as tf
import numpy as np
from tensorflow.python.keras.layers import  Dense, Dropout, Lambda, LeakyReLU, BatchNormalization
from tensorflow.python.keras import backend, Input, Model, optimizers

pi = tf.constant([3.1415927])  # pi 3.14
Nt = 64  # The number of transmit antennas


def dense_unit(input_tensor, nn):
    # input_tensor: the input tensor generated by the last keras layer
    # nn: the number of the neurons in the hidden layer.
    # out_tensor: the output tensor
    out_tensor = BatchNormalization()(input_tensor)
    out_tensor = Dense(nn, activation='relu')(out_tensor)
    return out_tensor


def loss_function_rate(y_true, y_pred):
    # y_true is actually the concatenation of the perfect CSI, h_real and the SNR, sigma^{-2}.
    # y_pred is the phases of obtained analog precoder v_RF.' (the transpose is due to the NN);
    h_real = tf.slice(y_true, [0,0], [-1, Nt])   # the real partition of the complex-valued h_real.
    h_imag = tf.slice(y_true, [0,Nt], [-1, Nt])  # the imaginary partition of the complex-valued h_real.
    signal_power = tf.slice(y_true, [0,Nt*2], [-1, 1])  # sigma^{-2}
    phase_vrf = tf.transpose(y_pred)  # the NN output is 1*Nt, but actual vrf is Nt*1.
    # transfer the y_pred (the phases) into exact complex v_RF (the lambda layer in the letter)
    v_real = tf.cos(phase_vrf)
    v_imag = tf.sin(phase_vrf)
    # compute the value of norm(hv_RF)^2
    # backend.batch_dot only compute the diagonal elements which is really required and thus reduce complexity.
    hvrf_2 = tf.pow(backend.batch_dot(h_real, v_real) - backend.batch_dot(h_imag, v_imag), 2) + \
        tf.pow(backend.batch_dot(h_real, v_imag) + backend.batch_dot(h_imag, v_real), 2)
    # compute the spectral efficiency with real CSI
    rate = tf.log(1 + hvrf_2 / Nt * signal_power) / tf.log(2.0)
    # since the NN is trained to minimize the objective, so we are aiming at minimizing the minus rate.
    return -rate


def bfnn_model():  # construct the BFNN proposed in this letter
    input_tensor = Input(shape=(2*Nt+1,))  # consists of the estimated CSI and the SNR value
    temp = dense_unit(input_tensor, 2048)  # the first dense unit with 2048 neurons
    temp = dense_unit(temp, 1024)  # the second dense unit with 1024 neurons
    temp = dense_unit(temp, 256)  # the third dense unit with 256 neurons
    out_phases = dense_unit(temp, Nt)  # to output the phases of v_RF
    model = Model(input_tensor, out_phases)
    model.compile(optimizer=tf.train.AdamOptimizer(), loss=loss_function_rate)
    print(model.summary())
    return model
